{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f1c290-3310-4930-bd34-fdb63ccd976f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (23.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-23.2.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.1.2\n",
      "    Uninstalling pip-23.1.2:\n",
      "      Successfully uninstalled pip-23.1.2\n",
      "Successfully installed pip-23.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pip --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caca9439-4750-4767-afbe-c1f1e2630d01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.167.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.174.0.tar.gz (857 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.4/857.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.26.157)\n",
      "Collecting boto3\n",
      "  Obtaining dependency information for boto3 from https://files.pythonhosted.org/packages/51/85/48dc639c57cb1a25b1f7b3022d2ab982fd97156715cb11f50102cb5c551f/boto3-1.28.19-py3-none-any.whl.metadata\n",
      "  Downloading boto3-1.28.19-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: awscli in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.27.157)\n",
      "Collecting awscli\n",
      "  Obtaining dependency information for awscli from https://files.pythonhosted.org/packages/fb/33/2ea211df886f1de36340eef7362da5b1e24573f4e8ac519d8e680ea7648f/awscli-1.29.19-py3-none-any.whl.metadata\n",
      "  Downloading awscli-1.29.19-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.24.3)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (3.20.3)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (4.13.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.5.3)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Collecting PyYAML~=6.0 (from sagemaker)\n",
      "  Obtaining dependency information for PyYAML~=6.0 from https://files.pythonhosted.org/packages/29/61/bf33c6c85c55bc45a29eee3195848ff2d518d84735eb0e2d8cb42e0d285e/PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (4.17.3)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (3.5.0)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.7.0)\n",
      "Collecting botocore<1.32.0,>=1.31.19 (from boto3)\n",
      "  Obtaining dependency information for botocore<1.32.0,>=1.31.19 from https://files.pythonhosted.org/packages/14/36/af5c6f6742773063d6729de739d331f5ed16a034a72242351868cc181e72/botocore-1.31.19-py3-none-any.whl.metadata\n",
      "  Downloading botocore-1.31.19-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3) (0.6.1)\n",
      "Requirement already satisfied: docutils<0.17,>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from awscli) (0.15.2)\n",
      "Requirement already satisfied: colorama<0.4.5,>=0.2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from awscli) (0.4.4)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from awscli) (4.7.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.19->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.19->boto3) (1.26.14)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.15.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.19.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Downloading boto3-1.28.19-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading awscli-1.29.19-py3-none-any.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.31.19-py3-none-any.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.5/705.5 kB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.174.0-py2.py3-none-any.whl size=1165493 sha256=2c2723ff9a869de8a896ded7df47c4db2526af2fd7e969eb621c4c5216fb60e6\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/f5/0c/4b/d428e250f6102a2e9f87a0d3c8db31bdd8548058c140ade18c\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: PyYAML, botocore, boto3, awscli, sagemaker\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 5.4.1\n",
      "    Uninstalling PyYAML-5.4.1:\n",
      "      Successfully uninstalled PyYAML-5.4.1\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.29.157\n",
      "    Uninstalling botocore-1.29.157:\n",
      "      Successfully uninstalled botocore-1.29.157\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.26.157\n",
      "    Uninstalling boto3-1.26.157:\n",
      "      Successfully uninstalled boto3-1.26.157\n",
      "  Attempting uninstall: awscli\n",
      "    Found existing installation: awscli 1.27.157\n",
      "    Uninstalling awscli-1.27.157:\n",
      "      Successfully uninstalled awscli-1.27.157\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.167.0\n",
      "    Uninstalling sagemaker-2.167.0:\n",
      "      Successfully uninstalled sagemaker-2.167.0\n",
      "Successfully installed PyYAML-6.0.1 awscli-1.29.19 boto3-1.28.19 botocore-1.31.19 sagemaker-2.174.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker boto3 awscli --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a505f33-64b1-4905-a5f4-6de0221ff840",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/ec2-user/.config/pip/pip.conf\n"
     ]
    }
   ],
   "source": [
    "!pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfc053ba-b138-4409-b7e7-674f8f9f48d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting neuronx-cc==2.*\n",
      "  Downloading https://pip.repos.neuron.amazonaws.com/neuronx-cc/neuronx_cc-2.8.0.25%2Ba3ad0f342-cp310-cp310-linux_x86_64.whl (362.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.6/362.6 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch-neuronx\n",
      "  Downloading https://pip.repos.neuron.amazonaws.com/torch-neuronx/torch_neuronx-1.13.1.1.9.0-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.0.0)\n",
      "Collecting torch\n",
      "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m984.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting neuronx-hwm==2.8.0.3 (from neuronx-cc==2.*)\n",
      "  Downloading https://pip.repos.neuron.amazonaws.com/neuronx-hwm/neuronx_hwm-2.8.0.3%2B2b7c6da39-cp310-cp310-linux_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx<=2.6.3 (from neuronx-cc==2.*)\n",
      "  Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy<=1.7.3 (from neuronx-cc==2.*)\n",
      "  Downloading scipy-1.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting python-daemon>=2.2.4 (from neuronx-cc==2.*)\n",
      "  Downloading python_daemon-3.0.1-py3-none-any.whl (31 kB)\n",
      "Collecting requests-unixsocket>=0.1.5 (from neuronx-cc==2.*)\n",
      "  Downloading requests_unixsocket-0.3.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: psutil>=5.6.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from neuronx-cc==2.*) (5.9.5)\n",
      "Collecting numpy<=1.21.6,>=1.20 (from neuronx-cc==2.*)\n",
      "  Downloading numpy-1.21.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting islpy<=2023.1,>2021.1 (from neuronx-cc==2.*)\n",
      "  Downloading islpy-2023.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: protobuf<4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from neuronx-cc==2.*) (3.20.3)\n",
      "Collecting pgzip>=0.3.0 (from neuronx-cc==2.*)\n",
      "  Obtaining dependency information for pgzip>=0.3.0 from https://files.pythonhosted.org/packages/61/9f/1a97b17fb29d7d1b6293faf13899c483460a2c524c3e06fe4226f6916133/pgzip-0.3.5-py3-none-any.whl.metadata\n",
      "  Downloading pgzip-0.3.5-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting ec2-metadata<=2.10.0 (from neuronx-cc==2.*)\n",
      "  Downloading ec2_metadata-2.10.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch-xla==1.13.1+torchneuron8 (from torch-neuronx)\n",
      "  Downloading https://pip.repos.neuron.amazonaws.com/torch-xla/torch_xla-1.13.1%2Btorchneuron8-cp310-cp310-linux_x86_64.whl (267.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.7/267.7 MB\u001b[0m \u001b[31m998.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting libneuronxla==0.5.391 (from torch-neuronx)\n",
      "  Downloading https://pip.repos.neuron.amazonaws.com/libneuronxla/libneuronxla-0.5.391-py3-none-linux_x86_64.whl (52.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.9/52.9 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch)\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m683.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch)\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting aws-neuronx-runtime-discovery~=2.0 (from libneuronxla==0.5.391->torch-neuronx)\n",
      "  Downloading https://pip.repos.neuron.amazonaws.com/aws-neuronx-runtime-discovery/aws-neuronx-runtime-discovery-2.9.tar.gz (1.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: boto3~=1.26 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from libneuronxla==0.5.391->torch-neuronx) (1.28.19)\n",
      "Requirement already satisfied: botocore~=1.29 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from libneuronxla==0.5.391->torch-neuronx) (1.31.19)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (67.7.2)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
      "Collecting absl-py>=1.0.0 (from torch-xla==1.13.1+torchneuron8->torch-neuronx)\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cloud-tpu-client>=0.10.0 (from torch-xla==1.13.1+torchneuron8->torch-neuronx)\n",
      "  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-xla==1.13.1+torchneuron8->torch-neuronx) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ec2-metadata<=2.10.0->neuronx-cc==2.*) (2.29.0)\n",
      "Requirement already satisfied: docutils in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-daemon>=2.2.4->neuronx-cc==2.*) (0.15.2)\n",
      "Collecting lockfile>=0.10 (from python-daemon>=2.2.4->neuronx-cc==2.*)\n",
      "  Downloading lockfile-0.12.2-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3~=1.26->libneuronxla==0.5.391->torch-neuronx) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3~=1.26->libneuronxla==0.5.391->torch-neuronx) (0.6.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore~=1.29->libneuronxla==0.5.391->torch-neuronx) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore~=1.29->libneuronxla==0.5.391->torch-neuronx) (1.26.14)\n",
      "Collecting google-api-python-client==1.8.0 (from cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuron8->torch-neuronx)\n",
      "  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting oauth2client (from cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuron8->torch-neuronx)\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httplib2<1dev,>=0.9.2 (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuron8->torch-neuronx)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth>=1.4.1 (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuron8->torch-neuronx)\n",
      "  Obtaining dependency information for google-auth>=1.4.1 from https://files.pythonhosted.org/packages/9c/8d/bff87fc722553a5691d8514da5523c23547f3894189ba03b57592e37bdc2/google_auth-2.22.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth-httplib2>=0.0.3 (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuron8->torch-neuronx)\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting google-api-core<2dev,>=1.13.0 (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuron8->torch-neuronx)\n",
      "  Downloading google_api_core-1.34.0-py3-none-any.whl (120 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.2/120.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six<2dev,>=1.6.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuron8->torch-neuronx) (1.16.0)\n",
      "Collecting uritemplate<4dev,>=3.0.0 (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuron8->torch-neuronx)\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->ec2-metadata<=2.10.0->neuronx-cc==2.*) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->ec2-metadata<=2.10.0->neuronx-cc==2.*) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->ec2-metadata<=2.10.0->neuronx-cc==2.*) (2023.5.7)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuron8->torch-neuronx) (0.4.8)\n",
      "Collecting pyasn1-modules>=0.0.5 (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuron8->torch-neuronx)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rsa>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuron8->torch-neuronx) (4.7.2)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2 (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuron8->torch-neuronx)\n",
      "  Obtaining dependency information for googleapis-common-protos<2.0dev,>=1.56.2 from https://files.pythonhosted.org/packages/a7/bc/416a1ffeba4dcd072bc10523dac9ed97f2e7fc4b760580e2bdbdc1e2afdd/googleapis_common_protos-1.60.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading googleapis_common_protos-1.60.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuron8->torch-neuronx)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuron8->torch-neuronx) (3.0.9)\n",
      "Downloading pgzip-0.3.5-py3-none-any.whl (13 kB)\n",
      "Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.8/181.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: aws-neuronx-runtime-discovery\n",
      "  Building wheel for aws-neuronx-runtime-discovery (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for aws-neuronx-runtime-discovery: filename=aws_neuronx_runtime_discovery-2.9-py3-none-any.whl size=1648 sha256=4e8a296358cc0398fdc7afd2c65b3b54061ac9e960e219ab1e4f544443e3dc4a\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/7e/4e/e9/b525c671fdb11523ddd29dd67f30b1648c110818f6478353a5\n",
      "Successfully built aws-neuronx-runtime-discovery\n",
      "Installing collected packages: lockfile, aws-neuronx-runtime-discovery, uritemplate, python-daemon, pyasn1-modules, pgzip, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, numpy, networkx, islpy, httplib2, googleapis-common-protos, cachetools, absl-py, scipy, requests-unixsocket, oauth2client, nvidia-cudnn-cu11, neuronx-hwm, google-auth, ec2-metadata, torch, neuronx-cc, google-auth-httplib2, google-api-core, google-api-python-client, libneuronxla, cloud-tpu-client, torch-xla, torch-neuronx\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.1\n",
      "    Uninstalling networkx-3.1:\n",
      "      Successfully uninstalled networkx-3.1\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.10.1\n",
      "    Uninstalling scipy-1.10.1:\n",
      "      Successfully uninstalled scipy-1.10.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.0\n",
      "    Uninstalling torch-2.0.0:\n",
      "      Successfully uninstalled torch-2.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "triton 2.0.0 requires cmake, which is not installed.\n",
      "triton 2.0.0 requires lit, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-1.4.0 aws-neuronx-runtime-discovery-2.9 cachetools-5.3.1 cloud-tpu-client-0.10 ec2-metadata-2.10.0 google-api-core-1.34.0 google-api-python-client-1.8.0 google-auth-2.22.0 google-auth-httplib2-0.1.0 googleapis-common-protos-1.60.0 httplib2-0.22.0 islpy-2023.1 libneuronxla-0.5.391 lockfile-0.12.2 networkx-2.6.3 neuronx-cc-2.8.0.25+a3ad0f342 neuronx-hwm-2.8.0.3+2b7c6da39 numpy-1.21.6 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 oauth2client-4.1.3 pgzip-0.3.5 pyasn1-modules-0.3.0 python-daemon-3.0.1 requests-unixsocket-0.3.0 scipy-1.7.3 torch-1.13.1 torch-neuronx-1.13.1.1.9.0 torch-xla-1.13.1+torchneuron8 uritemplate-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade neuronx-cc==2.* torch-neuronx torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0048c891-1749-46ad-9498-d2dba11be867",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded plugins: dkms-build-requires, extras_suggestions, langpacks, priorities,\n",
      "              : update-motd, versionlock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No Match for argument: aws-neuron-*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Packages marked for removal\n",
      "Loaded plugins: dkms-build-requires, extras_suggestions, langpacks, priorities,\n",
      "              : update-motd, versionlock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://download.docker.com/linux/centos/2/x86_64/stable/repodata/repomd.xml: [Errno 14] HTTPS Error 404 - Not Found\n",
      "Trying other mirror.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 packages excluded due to repository priority protections\n",
      "Package aws-neuronx-collectives-2.13.7.0_954e2e19b-1.x86_64 already installed and latest version\n",
      "Nothing to do\n",
      "Loaded plugins: dkms-build-requires, extras_suggestions, langpacks, priorities,\n",
      "              : update-motd, versionlock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://download.docker.com/linux/centos/2/x86_64/stable/repodata/repomd.xml: [Errno 14] HTTPS Error 404 - Not Found\n",
      "Trying other mirror.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 packages excluded due to repository priority protections\n",
      "Package aws-neuronx-runtime-lib-2.13.6.0_29de104d6-1.x86_64 already installed and latest version\n",
      "Nothing to do\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "sudo yum remove aws-neuron-* -y\n",
    "sudo yum install aws-neuronx-collectives-2.* -y\n",
    "sudo yum install aws-neuronx-runtime-lib-2.* -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "690c5dbf-9a66-459a-b670-de731d5c4416",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/21/02/ae8e595f45b6c8edee07913892b3b41f5f5f273962ad98851dc6a564bbb9/transformers-4.31.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.12.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.14.1 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/a4/06/85618f80ae552ac309ead9702c6826edda27884e26e07fdc8fa93f283546/regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.29.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n",
      "Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.4/770.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, safetensors, regex, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.16.4 regex-2023.6.3 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7a4a46c-1624-4db9-b1d4-9e3f4f9bcd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6496bdf9-bd63-494f-9805-ed72f64ca215",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-Aug-04 02:29:34.0706 11953:11953 ERROR  TDRV:tdrv_get_dev_info                       No neuron device available\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9fa0b10631a4f23bba4ae5fb32b9e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b80f838303c4bed81fcdaba5e79f4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014aba5a4b8d42d0b150387c6ad9029b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76cc9b43b8e4485b872ab7ef7a24ffab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa32c7be67148288391c65e94bd16e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "# Build tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\", \n",
    "                                                           return_dict=False)\n",
    "\n",
    "# Setup some example inputs\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "max_length=128\n",
    "paraphrase = tokenizer.encode_plus(sequence_0, \n",
    "                                   sequence_2, \n",
    "                                   max_length=max_length, \n",
    "                                   padding='max_length', \n",
    "                                   truncation=True, \n",
    "                                   return_tensors=\"pt\")\n",
    "\n",
    "not_paraphrase = tokenizer.encode_plus(sequence_0, \n",
    "                                       sequence_1, \n",
    "                                       max_length=max_length, \n",
    "                                       padding='max_length', \n",
    "                                       truncation=True, \n",
    "                                       return_tensors=\"pt\")\n",
    "\n",
    "# Run the original PyTorch model on compilation exaple\n",
    "paraphrase_classification_logits = model(**paraphrase)[0]\n",
    "\n",
    "# Convert example inputs to a format that is compatible with TorchScript tracing\n",
    "example_inputs_paraphrase = paraphrase['input_ids'], paraphrase['attention_mask'], paraphrase['token_type_ids']\n",
    "example_inputs_not_paraphrase = not_paraphrase['input_ids'], not_paraphrase['attention_mask'], not_paraphrase['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a77e8d71-9f06-4151-af71-2c4fcfa450dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "CPU times: user 7.97 s, sys: 3.54 s, total: 11.5 s\n",
      "Wall time: 43.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run torch_neuronx.trace to generate a TorchScript that is optimized by AWS Neuron\n",
    "model_neuron = torch_neuronx.trace(model, example_inputs_paraphrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b490bc3-8fe0-4184-a846-463c2e537512",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the TorchScript for later use\n",
    "model_neuron.save('neuron_compiled_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26615e93-6371-4824-b8ba-1413a4051157",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "neuron_compiled_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Now you'll create a model.tar.gz file to be used by SageMaker endpoint\n",
    "!tar -czvf model.tar.gz neuron_compiled_model.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dd9f5e1-5e84-409d-84be-9b186b776d4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::102048127330:role/service-role/SageMaker-ak-datascientist\n",
      "sagemaker bucket: sagemaker-us-west-2-102048127330\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, serializers, deserializers\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "sess_bucket = sess.default_bucket()\n",
    "\n",
    "print(f'sagemaker role arn: {role}')\n",
    "print(f'sagemaker bucket: {sess_bucket}')\n",
    "print(f'sagemaker session region: {sess.boto_region_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65a5fd14-9739-40a8-8f91-f4117cfcf4d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model artifcats uploaded to s3://sagemaker-us-west-2-102048127330/inf2_compiled_bert_model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "prefix = \"inf2_compiled_bert_model\"\n",
    "# create s3 uri\n",
    "s3_model_path = f\"s3://{sess_bucket}/{prefix}\"\n",
    "\n",
    "# upload model.tar.gz\n",
    "s3_model_uri = S3Uploader.upload(local_path=\"model.tar.gz\",desired_s3_uri=s3_model_path)\n",
    "print(f\"model artifcats uploaded to {s3_model_uri}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0023b9-5305-4c9d-b1b7-9ce585f73b50",
   "metadata": {},
   "source": [
    "### Deploy container and run inference based on the pretrained model \n",
    "To deploy a pretrained PyTorch model, you'll need to use the PyTorch estimator object to create a PyTorchModel object and set a different entry_point.\n",
    "\n",
    "You'll use the PyTorchModel object to deploy a PyTorchPredictor. This creates a SageMaker Endpoint -- a hosted prediction service that we can use to perform inference.\n",
    "\n",
    "An implementation of model_fn is required for inference script. We are going to implement our own model_fn and predict_fn for Hugging Face Bert, and use default implementations of input_fn and output_fn defined in sagemaker-pytorch-containers.\n",
    "\n",
    "In this example, the inference script is put in *code* folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6230b08d-6cc5-4816-9c98-5972e01da16d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"code\",exist_ok=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "314cb6ee-7fbf-4b16-8c01-42cb6cfaa4d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/requirements.txt\n",
    "\n",
    "transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c1764e0-ff80-46a5-88ca-e973fd2c206f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/inference.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch_neuronx\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "JSON_CONTENT_TYPE = 'application/json'\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    tokenizer_init = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "    model_file =os.path.join(model_dir, 'neuron_compiled_model.pt')\n",
    "    model_neuron = torch.jit.load(model_file)\n",
    "    return (model_neuron, tokenizer_init)\n",
    "\n",
    "\n",
    "def input_fn(serialized_input_data, content_type=JSON_CONTENT_TYPE):\n",
    "    if content_type == JSON_CONTENT_TYPE:\n",
    "        input_data = json.loads(serialized_input_data)\n",
    "        return input_data\n",
    "\n",
    "    else:\n",
    "        raise Exception('Requested unsupported ContentType in Accept: ' + content_type)\n",
    "        return\n",
    "\n",
    "\n",
    "def predict_fn(input_data, models):\n",
    "    model_bert, tokenizer = models\n",
    "    sequence_0 = input_data[0] \n",
    "    sequence_1 = input_data[1]\n",
    "    \n",
    "    max_length=128\n",
    "    paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    # Convert example inputs to a format that is compatible with TorchScript tracing\n",
    "    example_inputs_paraphrase = paraphrase['input_ids'], paraphrase['attention_mask'], paraphrase['token_type_ids']  \n",
    "\n",
    "    # Verify the TorchScript works on example inputs\n",
    "    paraphrase_classification_logits_neuron = model_bert(*example_inputs_paraphrase)\n",
    "    classes = ['not paraphrase', 'paraphrase']\n",
    "    paraphrase_prediction = paraphrase_classification_logits_neuron[0][0].argmax().item()\n",
    "    out_str = 'BERT says that \"{}\" and \"{}\" are {}'.format(sequence_0, sequence_1, classes[paraphrase_prediction])\n",
    "    \n",
    "    return out_str\n",
    "\n",
    "def output_fn(prediction_output, accept=JSON_CONTENT_TYPE):\n",
    "    if accept == JSON_CONTENT_TYPE:\n",
    "        return json.dumps(prediction_output), accept\n",
    "\n",
    "    raise Exception('Requested unsupported ContentType in Accept: ' + accept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00202d37-4bc5-4ab1-8bd2-3dbc8784a4d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-102048127330/inf2_compiled_bert_model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(s3_model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "359cc495-170c-42d9-b4e9-913ea2d88ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "ecr_image = f\"763104351884.dkr.ecr.{sess.boto_region_name}.amazonaws.com/pytorch-inference-neuronx:1.13.0-neuronx-py38-sdk2.9.0-ubuntu20.04\"\n",
    "\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=s3_model_uri,\n",
    "    role=role,\n",
    "    source_dir=\"code\",\n",
    "    entry_point=\"inference.py\",\n",
    "    image_uri = ecr_image,\n",
    ")\n",
    "\n",
    "# Let SageMaker know that we've already compiled the model via neuron-cc\n",
    "pytorch_model._is_compiled_model = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aeea52de-986f-4b8b-9492-f9a6d7d61e30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!CPU times: user 9.45 s, sys: 1.55 s, total: 11 s\n",
      "Wall time: 8min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predictor = pytorch_model.deploy(\n",
    "    instance_type=\"ml.inf2.xlarge\",\n",
    "    initial_instance_count=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69369a5e-4f2a-4849-8a08-c42f8f8991b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-inference-neuronx-ml-inf2-2023-08-04-02-40-10-685\n"
     ]
    }
   ],
   "source": [
    "print(predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5d35359-5e05-4669-8592-8194960d01db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48b619bd-3a2e-4005-829f-f3c39e182322",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT says that \"Never allow the same bug to bite you twice.\" and \"The best part of Amazon SageMaker is that it makes machine learning easy.\" are not paraphrase\n",
      "CPU times: user 18.5 ms, sys: 477 µs, total: 19 ms\n",
      "Wall time: 193 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = predictor.predict(\n",
    "    [\n",
    "        \"Never allow the same bug to bite you twice.\",\n",
    "        \"The best part of Amazon SageMaker is that it makes machine learning easy.\",\n",
    "    ]\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ada2b195-5d96-4287-bc96-9fdfa705ac9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT says that \"The company HuggingFace is based in New York City\" and \"HuggingFace's headquarters are situated in Manhattan\" are paraphrase\n",
      "CPU times: user 3.07 ms, sys: 0 ns, total: 3.07 ms\n",
      "Wall time: 19 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = predictor.predict(\n",
    "    [\n",
    "        \"The company HuggingFace is based in New York City\",\n",
    "        \"HuggingFace's headquarters are situated in Manhattan\",\n",
    "    ]\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09588912-1483-4dc3-bb7d-163ff445645c",
   "metadata": {},
   "source": [
    "## Let's run some benchmarks\n",
    "The following cells create a load test for your endpoint. You first define some helper functions: inference_latency runs the endpoint request, collects cliend side latency and any errors, random_sentence builds random to be sent to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e8f4868-58c4-4d1c-9820-fe95a3fae1e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference_latency(model, *inputs):\n",
    "    \"\"\"\n",
    "    infetence_time is a simple method to return the latency of a model inference.\n",
    "\n",
    "        Parameters:\n",
    "            model: torch model onbject loaded using torch.jit.load\n",
    "            inputs: model() args\n",
    "\n",
    "        Returns:\n",
    "            latency in seconds\n",
    "    \"\"\"\n",
    "    error = False\n",
    "    start = time.time()\n",
    "    try:\n",
    "        results = model(*inputs)\n",
    "    except:\n",
    "        error = True\n",
    "        results = []\n",
    "    return {'latency':time.time() - start, 'error': error, 'result': results}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d2579ba-5c1e-44f9-b911-5d2c403523e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A dude tries to automate the king', 'Superman hacks some guy']\n"
     ]
    }
   ],
   "source": [
    "def random_sentence():\n",
    "    \n",
    "    s_nouns = [\"A dude\", \"My mom\", \"The king\", \"Some guy\", \"A cat with rabies\", \"A sloth\", \"Your homie\", \"This cool guy my gardener met yesterday\", \"Superman\"]\n",
    "    p_nouns = [\"These dudes\", \"Both of my moms\", \"All the kings of the world\", \"Some guys\", \"All of a cattery's cats\", \"The multitude of sloths living under your bed\", \"Your homies\", \"Like, these, like, all these people\", \"Supermen\"]\n",
    "    s_verbs = [\"eats\", \"kicks\", \"gives\", \"treats\", \"meets with\", \"creates\", \"hacks\", \"configures\", \"spies on\", \"retards\", \"meows on\", \"flees from\", \"tries to automate\", \"explodes\"]\n",
    "    p_verbs = [\"eat\", \"kick\", \"give\", \"treat\", \"meet with\", \"create\", \"hack\", \"configure\", \"spy on\", \"retard\", \"meow on\", \"flee from\", \"try to automate\", \"explode\"]\n",
    "    infinitives = [\"to make a pie.\", \"for no apparent reason.\", \"because the sky is green.\", \"for a disease.\", \"to be able to make toast explode.\", \"to know more about archeology.\"]\n",
    "    \n",
    "    return (random.choice(s_nouns) + ' ' + random.choice(s_verbs) + ' ' + random.choice(s_nouns).lower() or random.choice(p_nouns).lower() + ' ' + random.choice(infinitives))\n",
    "\n",
    "print([random_sentence(), random_sentence()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "74bb563b-cbb3-45f0-bbfc-a9878b4a68e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435b84b68ee34c049c29ecc0146fe0c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Throughput: :301.1\n",
      "\n",
      "50th Percentile Latency:6.5 ms\n",
      "90th Percentile Latency:7.4 ms\n",
      "95th Percentile Latency:9.1 ms\n",
      "\n",
      "Errors percentage: 0.0 %\n",
      "\n",
      "Getting Cloudwatch:\n",
      "Time elapsed: 333.211178 seconds\n",
      "Using period of 360 seconds\n",
      "\n",
      "Waiting 30 seconds ...\n",
      "14061.0 latency datapoints ready\n",
      "50th Percentile Latency:2.6 ms\n",
      "90th Percentile Latency:3.3 ms\n",
      "95th Percentile Latency:3.7 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defining Auxiliary variables\n",
    "number_of_clients = 2\n",
    "number_of_runs = 10000\n",
    "t = tqdm(range(number_of_runs),position=0, leave=True)\n",
    "\n",
    "# Starting parallel clients\n",
    "cw_start = datetime.datetime.utcnow()\n",
    "\n",
    "results = Parallel(n_jobs=number_of_clients,prefer=\"threads\")(delayed(inference_latency)(predictor.predict,[random_sentence(), random_sentence()]) for mod in t)\n",
    "avg_throughput = t.total/t.format_dict['elapsed']\n",
    "\n",
    "cw_end = datetime.datetime.utcnow() \n",
    "\n",
    "# Computing metrics and print\n",
    "latencies = [res['latency'] for res in results]\n",
    "errors = [res['error'] for res in results]\n",
    "error_p = sum(errors)/len(errors) *100\n",
    "p50 = np.quantile(latencies[-10000:],0.50) * 1000\n",
    "p90 = np.quantile(latencies[-10000:],0.95) * 1000\n",
    "p95 = np.quantile(latencies[-10000:],0.99) * 1000\n",
    "\n",
    "print(f'Avg Throughput: :{avg_throughput:.1f}\\n')\n",
    "print(f'50th Percentile Latency:{p50:.1f} ms')\n",
    "print(f'90th Percentile Latency:{p90:.1f} ms')\n",
    "print(f'95th Percentile Latency:{p95:.1f} ms\\n')\n",
    "print(f'Errors percentage: {error_p:.1f} %\\n')\n",
    "\n",
    "# Querying CloudWatch\n",
    "print('Getting Cloudwatch:')\n",
    "cloudwatch = boto3.client('cloudwatch')\n",
    "statistics=['SampleCount', 'Average', 'Minimum', 'Maximum']\n",
    "extended=['p50', 'p90', 'p95', 'p100']\n",
    "\n",
    "# Give 5 minute buffer to end\n",
    "cw_end += datetime.timedelta(minutes=5)\n",
    "\n",
    "# Period must be 1, 5, 10, 30, or multiple of 60\n",
    "# Calculate closest multiple of 60 to the total elapsed time\n",
    "factor = math.ceil((cw_end - cw_start).total_seconds() / 60)\n",
    "period = factor * 60\n",
    "print('Time elapsed: {} seconds'.format((cw_end - cw_start).total_seconds()))\n",
    "print('Using period of {} seconds\\n'.format(period))\n",
    "\n",
    "cloudwatch_ready = False\n",
    "# Keep polling CloudWatch metrics until datapoints are available\n",
    "while not cloudwatch_ready:\n",
    "  time.sleep(30)\n",
    "  print('Waiting 30 seconds ...')\n",
    "  # Must use default units of microseconds\n",
    "  model_latency_metrics = cloudwatch.get_metric_statistics(MetricName='ModelLatency',\n",
    "                                             Dimensions=[{'Name': 'EndpointName',\n",
    "                                                          'Value': predictor.endpoint_name},\n",
    "                                                         {'Name': 'VariantName',\n",
    "                                                          'Value': \"AllTraffic\"}],\n",
    "                                             Namespace=\"AWS/SageMaker\",\n",
    "                                             StartTime=cw_start,\n",
    "                                             EndTime=cw_end,\n",
    "                                             Period=period,\n",
    "                                             Statistics=statistics,\n",
    "                                             ExtendedStatistics=extended\n",
    "                                             )\n",
    "\n",
    "  if len(model_latency_metrics['Datapoints']) > 0:\n",
    "    print('{} latency datapoints ready'.format(model_latency_metrics['Datapoints'][0]['SampleCount']))\n",
    "    side_avg = model_latency_metrics['Datapoints'][0]['Average'] / 1000\n",
    "    side_p50 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p50'] / 1000\n",
    "    side_p90 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p90'] / 1000\n",
    "    side_p95 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p95'] / 1000\n",
    "    side_p100 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p100'] / 1000\n",
    "    \n",
    "    print(f'50th Percentile Latency:{side_p50:.1f} ms')\n",
    "    print(f'90th Percentile Latency:{side_p90:.1f} ms')\n",
    "    print(f'95th Percentile Latency:{side_p95:.1f} ms\\n')\n",
    "\n",
    "    cloudwatch_ready = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad0bd04a-c0f3-4f5d-a84b-abd95e93d110",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## cleanup\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
